{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>traffic_volume</th>\n",
       "      <th>holiday</th>\n",
       "      <th>temp</th>\n",
       "      <th>rain_1h</th>\n",
       "      <th>snow_1h</th>\n",
       "      <th>clouds_all</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_in_week</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1462</td>\n",
       "      <td>No</td>\n",
       "      <td>243.39</td>\n",
       "      <td>No rain</td>\n",
       "      <td>No snow</td>\n",
       "      <td>Clear</td>\n",
       "      <td>Haze</td>\n",
       "      <td>haze</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1037</td>\n",
       "      <td>No</td>\n",
       "      <td>243.62</td>\n",
       "      <td>No rain</td>\n",
       "      <td>No snow</td>\n",
       "      <td>Clear</td>\n",
       "      <td>Haze</td>\n",
       "      <td>haze</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>800</td>\n",
       "      <td>No</td>\n",
       "      <td>244.22</td>\n",
       "      <td>No rain</td>\n",
       "      <td>No snow</td>\n",
       "      <td>Clear</td>\n",
       "      <td>Clear</td>\n",
       "      <td>sky is clear</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>354</td>\n",
       "      <td>No</td>\n",
       "      <td>244.82</td>\n",
       "      <td>No rain</td>\n",
       "      <td>No snow</td>\n",
       "      <td>Party Cloudy</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>few clouds</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>417</td>\n",
       "      <td>No</td>\n",
       "      <td>244.82</td>\n",
       "      <td>No rain</td>\n",
       "      <td>No snow</td>\n",
       "      <td>Party Cloudy</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>few clouds</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   traffic_volume holiday    temp  rain_1h  snow_1h    clouds_all  \\\n",
       "0            1462      No  243.39  No rain  No snow         Clear   \n",
       "1            1037      No  243.62  No rain  No snow         Clear   \n",
       "2             800      No  244.22  No rain  No snow         Clear   \n",
       "3             354      No  244.82  No rain  No snow  Party Cloudy   \n",
       "4             417      No  244.82  No rain  No snow  Party Cloudy   \n",
       "\n",
       "  weather_main weather_description  year  month  day  hour  day_in_week  \\\n",
       "0         Haze                haze  2016     12   18     8            6   \n",
       "1         Haze                haze  2016     12   18     7            6   \n",
       "2        Clear        sky is clear  2016     12   18     6            6   \n",
       "3       Clouds          few clouds  2013      2    2     3            5   \n",
       "4       Clouds          few clouds  2013      2    2     4            5   \n",
       "\n",
       "   is_weekend  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"C:/Users/Admin/OneDrive/VNU/4.1/3. Các hệ thống thông tin toàn cầu/Final/processed_data.csv\"\n",
    "data = pd.read_csv(path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traffic_volume         6\n",
      "holiday                6\n",
      "temp                   6\n",
      "rain_1h                6\n",
      "snow_1h                6\n",
      "clouds_all             6\n",
      "weather_main           6\n",
      "weather_description    6\n",
      "year                   6\n",
      "month                  6\n",
      "day                    6\n",
      "hour                   6\n",
      "day_in_week            6\n",
      "is_weekend             6\n",
      "dtype: int64\n",
      "traffic_volume         57\n",
      "holiday                57\n",
      "temp                   57\n",
      "rain_1h                57\n",
      "snow_1h                57\n",
      "clouds_all             57\n",
      "weather_main           57\n",
      "weather_description    57\n",
      "year                   57\n",
      "month                  57\n",
      "day                    57\n",
      "hour                   57\n",
      "day_in_week            57\n",
      "is_weekend             57\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "have_snow = data[data['snow_1h'] == 'Have snow']\n",
    "no_snow = data[data['snow_1h'] != 'Have snow']\n",
    "\n",
    "snow_train = have_snow.sample(frac=0.9, random_state=42) \n",
    "snow_test = have_snow.drop(snow_train.index)\n",
    "\n",
    "print(snow_test.count())\n",
    "print(snow_train.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split data by condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data):\n",
    "  \"\"\"\n",
    "    split data based on snow_1h and rain_1h columns.\n",
    "    \n",
    "    Steps:\n",
    "    1. 90% of rows where 'snow_1h' == 'Have snow' go to train, 10% to test.\n",
    "    2. from the remaining training data, 90% of rows where 'rain_1h' == 'Have rain' stay in train, 10% move to test.\n",
    "    3. split continue in train dataset\n",
    "    \n",
    "  \"\"\"\n",
    "  # define snow for handle imbalance\n",
    "  have_snow = data[data['snow_1h'] == 'Have snow']\n",
    "  no_snow = data[data['snow_1h'] != 'Have snow']\n",
    "  \n",
    "  # split by snow\n",
    "  snow_train = have_snow.sample(frac=0.9, random_state=42) \n",
    "  snow_test = have_snow.drop(snow_train.index)\n",
    "  \n",
    "  train_data = pd.concat([no_snow, snow_train])\n",
    "  test_data = snow_test\n",
    "  \n",
    "  # define value in rain for handle imbalance\n",
    "  have_rain = data[data['rain_1h'] == 'Have rain']\n",
    "  no_rain = data[data['rain_1h'] == 'No rain']\n",
    "  \n",
    "  # continue slit with rain\n",
    "  rain_train = have_rain.sample(frac=0.9, random_state=42) \n",
    "  rain_test = have_rain.drop(rain_train.index)  \n",
    "  \n",
    "  # train & test after split by columns\n",
    "  train_data = pd.concat([no_rain, rain_train])\n",
    "  test_data = pd.concat([test_data, rain_test])\n",
    "  \n",
    "  # Split in train set to get enough value for testing\n",
    "  final_train = train_data.sample(frac=0.8, random_state=42)  \n",
    "  validation = train_data.drop(final_train.index)  \n",
    "  test_data = pd.concat([test_data, validation])\n",
    "\n",
    "  return final_train, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset length:  38277\n",
      "test data length:  9922\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = split_dataset(data)\n",
    "print(f\"train dataset length: \", len(train_data))\n",
    "print(f\"test data length: \", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "def one_hot_encoding(df):\n",
    "    # Include only category columns\n",
    "    category_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Create a OneHotEncoder instance\n",
    "    onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    \n",
    "    # Fit and transform the categorical columns\n",
    "    encoded_df = df.copy()\n",
    "    onehot_encoded = onehot_encoder.fit_transform(df[category_columns])\n",
    "    \n",
    "    # Create a DataFrame with the one-hot encoded data\n",
    "    onehot_encoded_df = pd.DataFrame(onehot_encoded, columns=onehot_encoder.get_feature_names_out(category_columns))\n",
    "    \n",
    "    # Drop the original categorical columns and concatenate the one-hot encoded columns\n",
    "    encoded_df = encoded_df.drop(columns=category_columns).reset_index(drop=True)\n",
    "    encoded_df = pd.concat([encoded_df, onehot_encoded_df], axis=1)\n",
    "    \n",
    "    return encoded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def random_forest_train(train_data, target_column, feature_columns, hyperparameters):\n",
    "    # Identify categorical columns\n",
    "    categorical_columns = train_data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Define the column transformer with OneHotEncoder for categorical features\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    X_train = train_data[feature_columns]\n",
    "    y_train = train_data[target_column]\n",
    "    \n",
    "    # Set up the RandomForestRegressor with specified hyperparameters\n",
    "    rf = RandomForestRegressor(**hyperparameters, random_state=42)\n",
    "    \n",
    "    # Create a pipeline that first applies the preprocessor and then the RandomForestRegressor\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('rf', rf)])\n",
    "    \n",
    "    # Fit the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    return pipeline, hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, train_data, test_data, target_column, feature_columns):\n",
    "    X_train = train_data[feature_columns]\n",
    "    y_train = train_data[target_column]\n",
    "    X_test = test_data[feature_columns]\n",
    "    y_test = test_data[target_column]\n",
    "    \n",
    "    # Calculate R2 on training data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    # Calculate RMSE on test data\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    return r2_train, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model dumping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saving random_forest_model.joblib successfully executed'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "def save_file(scaler, filename):\n",
    "    joblib.dump(scaler, filename)\n",
    "    return f\"Saving {filename} successfully executed\"\n",
    "\n",
    "save_file(model,'random_forest_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = train_data.columns.drop('traffic_volume').tolist()\n",
    "hyperparameters = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 10,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features': 'sqrt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Hyperparameters: {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "\n",
      "R2 on Train-set: 0.686393924554298\n",
      "\n",
      "Root Mean Squared Error (RMSE) on Test Set: 1124.0897916449273\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model, best_params = random_forest_train(train_data, target_column=\"traffic_volume\", feature_columns=feature_columns, hyperparameters=hyperparameters)\n",
    "\n",
    "# Evaluate the model\n",
    "r2_train, rmse = evaluate_model(model, train_data, test_data, target_column=\"traffic_volume\", feature_columns=feature_columns)\n",
    "\n",
    "# Print the results\n",
    "print(f\"\\nBest Hyperparameters: {best_params}\")\n",
    "print(f\"\\nR2 on Train-set: {r2_train}\")\n",
    "print(f\"\\nRoot Mean Squared Error (RMSE) on Test Set: {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
